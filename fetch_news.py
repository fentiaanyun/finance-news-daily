#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è´¢ç»æ–°é—»æŠ“å–å’Œæ¨é€è„šæœ¬
æŠ“å–è¿‡å»24å°æ—¶çš„è´¢ç»æ–°é—»ï¼Œé€šè¿‡Serveré…±æ¨é€åˆ°å¾®ä¿¡ï¼Œå¹¶å‘é€é‚®ä»¶
"""

import requests
import json
from datetime import datetime, timedelta
import os
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.header import Header
import feedparser
import re
import time

# è¯·æ±‚å¤´ï¼Œé¿å…è¢«éƒ¨åˆ†ç«™ç‚¹æ‹’ç»
REQUEST_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'application/rss+xml, application/xml, text/xml, */*',
}

def fetch_news_from_rss():
    """ä»RSSæºæŠ“å–è´¢ç»æ–°é—»ï¼ˆè´¢ç»æºç›´æ¥å–æœ€æ–°æ¡ç›®ï¼Œä¸ä¾èµ–å…³é”®è¯è¿‡æ»¤ï¼‰"""
    news_list = []
    
    # RSSæºåˆ—è¡¨ - ä½¿ç”¨å…¨çƒå¯è®¿é—®çš„ã€ç¨³å®šçš„æº
    rss_sources = [
        # CNBC (ç¾å›½è´¢ç»ç½‘ç»œç”µè§†)
        {'url': 'https://www.cnbc.com/id/100003114/device/rss/rss.html', 'source': 'CNBC', 'priority': 1},
        
        # Yahoo Finance
        {'url': 'https://feeds.finance.yahoo.com/rss/2.0/headline', 'source': 'Yahoo Finance', 'priority': 1},
        
        # Investor's Business Daily
        {'url': 'https://feeds.investors.com/feeds/ibd-top-10.xml', 'source': 'IBD', 'priority': 1},
        
        # Financial Express (å°åº¦è´¢ç»)
        {'url': 'https://www.financialexpress.com/feed/', 'source': 'Financial Express', 'priority': 2},
        
        # Business Insider
        {'url': 'https://feeds.businessinsider.com/markets/news', 'source': 'Business Insider', 'priority': 2},
        
        # The Motley Fool
        {'url': 'https://feeds.fool.com/foolscoop/index.xml', 'source': 'The Motley Fool', 'priority': 2},
    ]
    
    # å¼ºåŒ–å…³é”®è¯ï¼ˆä¸­è‹±æ–‡æ··åˆï¼Œé€‚åº”å›½é™…è´¢ç»æ–°é—»ï¼‰
    keywords = [
        # å¤®è¡Œç›¸å…³
        'ç¾è”å‚¨', 'Federal Reserve', 'Fed', 'interest rate', 'å¤®è¡Œ', 'central bank', 'monetary policy', 'é™æ¯', 'åŠ æ¯',
        
        # æ”¿æ²»ç»æµ
        'Trump', 'ç‰¹æœ—æ™®', 'President', 'æ€»ç»Ÿ', 'æ”¿åºœ', 'government', 'policy', 'æ”¿ç­–',
        
        # è‚¡å¸‚
        'stock', 'è‚¡ç¥¨', 'stock market', 'è‚¡å¸‚', 'index', 'æŒ‡æ•°', 'Nasdaq', 'NYSE', 'Dow Jones',
        'S&P 500', 'ä¸Šè¯', 'æ·±è¯', 'åˆ›ä¸šæ¿', 'Shanghai', 'Shenzhen',
        
        # ç»æµæŒ‡æ ‡
        'inflation', 'é€šèƒ€', 'GDP', 'unemployment', 'å°±ä¸š', 'CPI', 'PPI', 'ç»æµ',
        'earnings', 'ç›ˆåˆ©', 'revenue', 'æ”¶å…¥', 'profit', 'åˆ©æ¶¦',
        
        # è¡Œä¸šä¸å…¬å¸
        'Tesla', 'Apple', 'Microsoft', 'Amazon', 'Google',
        'Bitcoin', 'æ¯”ç‰¹å¸', 'crypto', 'åŠ å¯†', 'technology', 'ç§‘æŠ€',
        'energy', 'èƒ½æº', 'oil', 'æ²¹ä»·', 'banking', 'é“¶è¡Œ',
        
        # æ±‡ç‡å¤–æ±‡
        'exchange rate', 'æ±‡ç‡', 'yuan', 'äººæ°‘å¸', 'dollar', 'euro', 'æ¬§å…ƒ',
        
        # å€ºåˆ¸å¸‚åœº
        'bond', 'å€ºåˆ¸', 'Treasury', 'å›½å€º', 'yield', 'æ”¶ç›Šç‡',
        
        # è´¸æ˜“ä¸å…³ç¨
        'trade', 'è´¸æ˜“', 'tariff', 'å…³ç¨', 'commerce', 'å•†ä¸š',
        
        # æˆ¿åœ°äº§
        'real estate', 'æˆ¿åœ°äº§', 'property', 'æˆ¿äº§', 'housing', 'ä½æˆ¿',
        
        # å…¶ä»–å¸¸è§è¯
        'market', 'å¸‚åœº', 'price', 'ä»·æ ¼', 'rise', 'ä¸Šå‡', 'fall', 'ä¸‹é™',
        'gain', 'loss', 'æ¶¨', 'è·Œ', 'surge', 'é£™å‡', 'crash', 'å´©ç›˜'
    ]
    
    
    for rss_info in rss_sources:
        try:
            print(f"å°è¯•æŠ“å– {rss_info['source']} (ä¼˜å…ˆçº§{rss_info['priority']})...")
            # ç”¨ requests æ‹‰å–ï¼Œå¸¦ User-Agentï¼Œå†äº¤ç»™ feedparser è§£æ
            try:
                resp = requests.get(
                    rss_info['url'], 
                    headers=REQUEST_HEADERS, 
                    timeout=20,  # å¢åŠ è¶…æ—¶æ—¶é—´
                    verify=False  # å¿½ç•¥SSLéªŒè¯
                )
                resp.raise_for_status()
                resp.encoding = resp.apparent_encoding or 'utf-8'
                feed = feedparser.parse(resp.content)
            except requests.exceptions.Timeout:
                print(f"  {rss_info['source']} è¿æ¥è¶…æ—¶ï¼Œè·³è¿‡")
                continue
            except requests.exceptions.ConnectionError as e:
                print(f"  {rss_info['source']} ç½‘ç»œé”™è¯¯: {str(e)[:50]}...")
                continue
            except Exception as e:
                print(f"  {rss_info['source']} è·å–å¤±è´¥: {str(e)[:50]}...")
                continue
            
            if not feed.entries:
                print(f"  {rss_info['source']} æ— æ¡ç›®")
                continue
                
            current_time = datetime.now()
            taken = 0
            max_per_source = 15  # æ‰©å¤§å•æºè·å–æ•°é‡
            
            for entry in feed.entries:
                if taken >= max_per_source:
                    break
                title = entry.get('title', '').strip()
                if not title:
                    continue
                
                # è·å–å‘å¸ƒæ—¶é—´ï¼ˆå¯é€‰ï¼Œä½†ä¸å¼ºåˆ¶è¿‡æ»¤ï¼‰
                pub_time = None
                try:
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_time = datetime(*entry.published_parsed[:6])
                        # æ”¾å®½åˆ°72å°æ—¶ï¼Œä½†ä¼˜å…ˆæ˜¾ç¤º24å°æ—¶å†…çš„
                        if current_time - pub_time > timedelta(hours=72):
                            continue
                except Exception:
                    pass  # å¦‚æœæ— æ³•è§£ææ—¶é—´ï¼Œä»ç„¶ä¿ç•™
                
                summary = entry.get('summary', '') or entry.get('description', '')
                if summary and hasattr(summary, 'replace'):
                    summary = re.sub(r'<[^>]+>', '', summary)
                else:
                    summary = ''
                
                content = (title + ' ' + summary)
                # æ”¹è¿›å…³é”®è¯æ£€æµ‹ï¼šä»»æ„åŒ¹é…éƒ½ç®—"é‡è¦"
                is_highlight = any(kw in content for kw in keywords)
                # å¦‚æœæ²¡æœ‰å…³é”®è¯ä½†æœ‰å†…å®¹ï¼Œä»ç„¶ä¿ç•™ï¼ˆé‡è¦æ€§ç¨ä½ï¼‰
                
                news_item = {
                    'title': title,
                    'description': (summary[:200] if summary else ''),
                    'url': entry.get('link', ''),
                    'source': rss_info['source'],
                    'publishedAt': entry.get('published', ''),
                    'highlight': is_highlight,
                    'time_diff': (current_time - pub_time).total_seconds() / 3600 if pub_time else None,
                }
                news_list.append(news_item)
                taken += 1
            
            print(f"  è·å– {taken} æ¡æ–°é—»")
        except Exception as e:
            print(f"  {rss_info['source']} å¤„ç†å¼‚å¸¸: {str(e)[:50]}...")
            continue
    
    # è‹¥ä¸€æ¡éƒ½æ²¡æœ‰ï¼Œåšå…œåº•ï¼šä¸é™åˆ¶æ—¶é—´ï¼Œä»èƒ½ç”¨çš„æºå–æœ€æ–°å‡ æ¡
    if not news_list:
        print("\nâš ï¸ ä¸»åˆ—è¡¨ä¸ºç©ºï¼Œæ‰§è¡Œå…œåº•ç­–ç•¥...")
        for rss_info in rss_sources:
            try:
                print(f"  å…œåº•å°è¯•: {rss_info['source']}")
                resp = requests.get(
                    rss_info['url'], 
                    headers=REQUEST_HEADERS, 
                    timeout=20,
                    verify=False
                )
                resp.raise_for_status()
                resp.encoding = resp.apparent_encoding or 'utf-8'
                feed = feedparser.parse(resp.content)
                
                for entry in (feed.entries or [])[:20]:  # å…œåº•å¢åŠ åˆ°20æ¡
                    title = entry.get('title', '').strip()
                    if not title:
                        continue
                    summary = entry.get('summary', '') or entry.get('description', '')
                    if summary and hasattr(summary, 'replace'):
                        summary = re.sub(r'<[^>]+>', '', summary)
                    else:
                        summary = ''
                    news_list.append({
                        'title': title,
                        'description': (summary[:200] if summary else ''),
                        'url': entry.get('link', ''),
                        'source': rss_info['source'],
                        'publishedAt': entry.get('published', ''),
                        'highlight': False,
                        'time_diff': None,
                    })
                
                if news_list:
                    print(f"  âœ“ å…œåº•è·å– {len(news_list)} æ¡æ–°é—»")
                    break
            except Exception as e:
                print(f"  âœ— å…œåº•å¤±è´¥ {rss_info['source']}: {str(e)[:30]}...")
                continue
    
    # ä¼˜å…ˆå±•ç¤ºå«å…³é”®è¯çš„ï¼Œå†æŒ‰æ—¶é—´æ’åº
    news_list.sort(key=lambda x: (
        not x.get('highlight', False),  # ä¼˜å…ˆæ˜¾ç¤ºæœ‰å…³é”®è¯çš„
        -(x.get('time_diff', 999) or 999)  # å†æŒ‰æ—¶é—´æ–°æ—§æ’åº
    ))
    
    return news_list

def fetch_news_from_web_scraping():
    """å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥ä»ç½‘é¡µçˆ¬å–è´¢ç»æ–°é—»"""
    news_list = []
    
    # çˆ¬å–çš„ç½‘ç«™åˆ—è¡¨ - å…¨çƒå¯è®¿é—®çš„è´¢ç»ç½‘ç«™
    scrape_sources = [
        {
            'url': 'https://news.google.com/topics/CAAqKggKEhAP-nZ_GqJEFQryfS9NqMEsqAEwkqKBBigBKkAP-nZ_GqJEFQryfS9NqMEsqAEwkqKBBg',
            'name': 'Google News - Business',
            'parser': 'simple'
        },
        {
            'url': 'https://www.cnbc.com/markets/',
            'name': 'CNBC Markets',
            'parser': 'simple'
        },
        {
            'url': 'https://www.bloomberg.com/markets',
            'name': 'Bloomberg Markets',
            'parser': 'simple'
        }
    ]
    
    from bs4 import BeautifulSoup
    
    for source in scrape_sources:
        try:
            print(f"  å°è¯•çˆ¬å– {source['name']}...")
            response = requests.get(
                source['url'],
                headers=REQUEST_HEADERS,
                timeout=15,
                verify=False
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # é€šç”¨æ–°é—»é¡¹æå–
            articles = soup.find_all('article')[:10] or soup.find_all('a', {'data-article-id': True})[:10]
            
            for article in articles[:10]:
                try:
                    title = None
                    link = None
                    
                    # å°è¯•å¤šç§é€‰æ‹©å™¨
                    title_elem = article.find('h2') or article.find('h3') or article.find('a')
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                    
                    if not title:
                        continue
                    
                    link_elem = article.find('a', href=True)
                    if link_elem:
                        link = link_elem['href']
                        if not link.startswith('http'):
                            link = 'https://' + source['url'].split('/')[2] + link
                    
                    if title and len(title) > 5:
                        news_list.append({
                            'title': title[:200],
                            'description': '',
                            'url': link or '',
                            'source': source['name'],
                            'publishedAt': '',
                            'highlight': True,
                            'time_diff': 0,
                        })
                except Exception:
                    continue
            
            if len(news_list) >= 5:
                print(f"  âœ“ çˆ¬å–åˆ° {len(news_list)} æ¡æ–°é—»")
                break
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {str(e)[:30]}...")
            continue
    
    return news_list

def fetch_news_from_api():
    """ä»NewsAPIæŠ“å–æ–°é—»ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰"""
    news_list = []
    api_key = os.getenv('NEWS_API_KEY', '')
    
    if not api_key:
        print("æœªé…ç½®NEWS_API_KEYï¼Œè·³è¿‡APIæ–°é—»æº")
        return news_list
    
    keywords = [
        'Federal Reserve',  # ç¾è”å‚¨
        'US President',     # ç¾å›½æ€»ç»Ÿ
        'interest rate',     # åˆ©ç‡
        'inflation',         # é€šèƒ€
        'stock market',      # è‚¡å¸‚
        'exchange rate',     # æ±‡ç‡
        'central bank',      # å¤®è¡Œ
        'monetary policy'    # è´§å¸æ”¿ç­–
    ]
    
    try:
        for keyword in keywords:
            url = "https://newsapi.org/v2/everything"
            params = {
                'q': keyword,
                'sortBy': 'publishedAt',
                'language': 'en',
                'apiKey': api_key,
                'from': (datetime.now() - timedelta(hours=24)).strftime('%Y-%m-%dT%H:%M:%S'),
                'pageSize': 5
            }
            
            response = requests.get(url, params=params, timeout=10)
            if response.status_code == 200:
                articles = response.json().get('articles', [])
                for article in articles:
                    news_list.append({
                        'title': article.get('title', ''),
                        'description': article.get('description', '')[:200] if article.get('description') else '',
                        'url': article.get('url', ''),
                        'source': article.get('source', {}).get('name', 'NewsAPI'),
                        'publishedAt': article.get('publishedAt', '')
                    })
            
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
    except Exception as e:
        print(f"APIæŠ“å–å¤±è´¥: {e}")
    
    return news_list

def format_news_content(news_list):
    """æ ¼å¼åŒ–æ–°é—»å†…å®¹ä¸ºæ¨é€æ ¼å¼"""
    if not news_list:
        return "ğŸ“° è¿‡å»24å°æ—¶æœªå‘ç°é‡è¦è´¢ç»æ–°é—»ï¼Œä½†ç³»ç»Ÿæ­£å¸¸è¿è¡Œã€‚å¦‚æœæŒç»­æ— æ–°é—»ï¼Œè¯·æ£€æŸ¥RSSæºæ˜¯å¦å¯è®¿é—®ã€‚"
    
    # å»é‡ï¼ˆåŸºäºæ ‡é¢˜ï¼‰
    seen_titles = set()
    unique_news = []
    for news in news_list:
        title = news.get('title', '')
        if title and title not in seen_titles:
            seen_titles.add(title)
            unique_news.append(news)
    
    # ç»Ÿè®¡ä¿¡æ¯
    highlight_count = sum(1 for n in unique_news if n.get('highlight'))
    
    # æŒ‰æ¥æºåˆ†ç»„
    content = f"ğŸ“Š è´¢ç»æ—©æŠ¥ - {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n"
    content += "=" * 50 + "\n\n"
    content += f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯\n"
    content += f"  â€¢ æ€»æ–°é—»æ•°ï¼š{len(unique_news)} æ¡\n"
    content += f"  â€¢ é‡è¦æ–°é—»ï¼š{highlight_count} æ¡\n"
    content += f"  â€¢ æ›´æ–°æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    
    if len(unique_news) == 0:
        content += "âš ï¸ æš‚æ— æ–°é—»æ•°æ®\n"
        return content
    
    # æŒ‰é‡è¦æ€§å’Œæ¥æºåˆ†ç±»æ˜¾ç¤º
    highlight_news = [n for n in unique_news if n.get('highlight')]
    normal_news = [n for n in unique_news if not n.get('highlight')]
    
    # å…ˆæ˜¾ç¤ºé‡è¦æ–°é—»
    if highlight_news:
        content += "â­ ã€é‡è¦æ–°é—»ã€‘\n"
        content += "-" * 50 + "\n"
        for idx, news in enumerate(highlight_news[:10], 1):  # æ˜¾ç¤ºå‰10æ¡é‡è¦æ–°é—»
            title = news.get('title', 'æ— æ ‡é¢˜')
            description = news.get('description', '')
            url = news.get('url', '')
            source = news.get('source', 'æœªçŸ¥æ¥æº')
            time_info = ""
            if news.get('time_diff') is not None:
                hours = int(news.get('time_diff', 0))
                if hours < 1:
                    time_info = " (åˆšåˆš)"
                elif hours < 24:
                    time_info = f" ({hours}å°æ—¶å‰)"
                else:
                    time_info = f" ({hours//24}å¤©å‰)"
            
            content += f"{idx}. ã€{source}ã€‘{title}{time_info}\n"
            if description:
                content += f"   {description}...\n"
            if url:
                content += f"   ğŸ”— {url}\n"
            content += "\n"
    
    # å†æ˜¾ç¤ºæ™®é€šæ–°é—»
    if normal_news:
        remaining = min(10, len(normal_news))  # æœ€å¤šå†æ˜¾ç¤º10æ¡
        content += f"\nğŸ“° ã€å…¶ä»–æ–°é—»ã€‘(æ˜¾ç¤ºå‰{remaining}æ¡)\n"
        content += "-" * 50 + "\n"
        for idx, news in enumerate(normal_news[:remaining], 1):
            title = news.get('title', 'æ— æ ‡é¢˜')
            source = news.get('source', 'æœªçŸ¥æ¥æº')
            time_info = ""
            if news.get('time_diff') is not None:
                hours = int(news.get('time_diff', 0))
                if hours < 1:
                    time_info = " (åˆšåˆš)"
                elif hours < 24:
                    time_info = f" ({hours}å°æ—¶å‰)"
            
            content += f"{idx}. ã€{source}ã€‘{title}{time_info}\n"
        
        if len(normal_news) > remaining:
            content += f"\n...è¿˜æœ‰ {len(normal_news) - remaining} æ¡æ–°é—»æœªæ˜¾ç¤º\n"
    
    content += "\n" + "=" * 50 + "\n"
    content += "âœ¨ æ›´å¤šè´¢ç»èµ„è®¯è¯·è®¿é—®ï¼šæ–°æµªè´¢ç»ã€ä¸œæ–¹è´¢å¯Œ\n"
    
    return content

def send_via_serverchan(content):
    """é€šè¿‡Serveré…±APIæ¨é€åˆ°å¾®ä¿¡"""
    send_key = os.getenv('SERVERCHAN_SEND_KEY', '')
    
    if not send_key:
        print("âš ï¸ æœªé…ç½®SERVERCHAN_SEND_KEYï¼Œè·³è¿‡å¾®ä¿¡æ¨é€")
        print("   è¯·åœ¨GitHub Secretsä¸­æ·»åŠ  SERVERCHAN_SEND_KEY")
        print("   è·å–æ–¹å¼ï¼šè®¿é—® https://sctapi.ftqq.com/ å¾®ä¿¡æ‰«ç ç™»å½•è·å–")
        return False
    
    # Serveré…±æœ€æ–°APIåœ°å€
    url = f"https://sctapi.ftqq.com/{send_key}.send"
    
    title = f"ğŸ“Š è´¢ç»æ—©æŠ¥ - {datetime.now().strftime('%mæœˆ%dæ—¥')}"
    
    data = {
        'title': title,
        'desp': content
    }
    
    try:
        print(f"æ­£åœ¨æ¨é€åˆ°Serveré…±: {url[:50]}...")
        response = requests.post(url, data=data, timeout=10)
        result = response.json()
        
        if response.status_code == 200 and result.get('code') == 0:
            print("âœ… å¾®ä¿¡æ¨é€æˆåŠŸ - å·²å‘é€åˆ°å¾®ä¿¡")
            return True
        else:
            error_msg = result.get('message', 'æœªçŸ¥é”™è¯¯')
            error_code = result.get('code', 'N/A')
            print(f"âŒ å¾®ä¿¡æ¨é€å¤±è´¥")
            print(f"   é”™è¯¯ä»£ç : {error_code}")
            print(f"   é”™è¯¯ä¿¡æ¯: {error_msg}")
            
            # å¸¸è§é”™è¯¯è¯Šæ–­
            if error_code == 40001:
                print("   ğŸ’¡ æç¤ºï¼šSendKeyå¯èƒ½å·²è¿‡æœŸï¼Œè¯·é‡æ–°è·å–")
            elif error_code == 40002:
                print("   ğŸ’¡ æç¤ºï¼šSendKeyæ ¼å¼é”™è¯¯")
            
            return False
    except Exception as e:
        print(f"âŒ å¾®ä¿¡æ¨é€å¼‚å¸¸: {e}")
        print(f"   è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–Serveré…±æœåŠ¡æ˜¯å¦å¯ç”¨")
        return False

def send_email(content, recipient_email):
    """å‘é€é‚®ä»¶ï¼ˆQQé‚®ç®±ä¼˜å…ˆä½¿ç”¨465ç«¯å£SSLï¼Œå¤±è´¥åˆ™å°è¯•587 TLSï¼‰"""
    sender_email = os.getenv('EMAIL_SENDER', '')
    email_password = os.getenv('EMAIL_PASSWORD', '')
    smtp_server = os.getenv('SMTP_SERVER', 'smtp.qq.com')
    smtp_port = int(os.getenv('SMTP_PORT', '587'))
    
    if not sender_email or not email_password:
        print("âš ï¸ æœªé…ç½®é‚®ç®±ä¿¡æ¯")
        print("   éœ€è¦é…ç½®çš„ç¯å¢ƒå˜é‡:")
        print("   â€¢ EMAIL_SENDER: å‘é€è€…é‚®ç®±åœ°å€")
        print("   â€¢ EMAIL_PASSWORD: é‚®ç®±å¯†ç æˆ–æˆæƒç ")
        print("   â€¢ EMAIL_RECIPIENT: æ¥æ”¶è€…é‚®ç®±åœ°å€")
        print("   â€¢ SMTP_SERVER: SMTPæœåŠ¡å™¨åœ°å€ (é»˜è®¤: smtp.qq.com)")
        print("   â€¢ SMTP_PORT: SMTPæœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 587)")
        return False
    
    if not recipient_email:
        recipient_email = sender_email  # é»˜è®¤å‘é€ç»™è‡ªå·±
        print(f"ğŸ“§ å‘é€ç»™: {recipient_email} (é»˜è®¤å‘é€ç»™è‡ªå·±)")
    else:
        print(f"ğŸ“§ å‘é€ç»™: {recipient_email}")
    
    subject = f"è´¢ç»æ—©æŠ¥ - {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}"
    html_content = content.replace('\n', '<br>')
    msg = MIMEMultipart()
    msg['From'] = sender_email
    msg['To'] = recipient_email
    msg['Subject'] = Header(subject, 'utf-8')
    msg.attach(MIMEText(html_content, 'html', 'utf-8'))
    
    print(f"   SMTPæœåŠ¡å™¨: {smtp_server}:{smtp_port}")
    
    # å…ˆå°è¯• 465 ç«¯å£ SSLï¼ˆQQé‚®ç®±æ›´ç¨³å®šï¼‰
    if smtp_server == 'smtp.qq.com':
        try:
            print("   å°è¯•ä½¿ç”¨ 465 ç«¯å£ SSL è¿æ¥...")
            server = smtplib.SMTP_SSL(smtp_server, 465, timeout=15)
            server.login(sender_email, email_password)
            server.sendmail(sender_email, [recipient_email], msg.as_string())
            server.quit()
            print("   âœ… é‚®ä»¶å‘é€æˆåŠŸ (465 SSL)")
            return True
        except smtplib.SMTPAuthenticationError:
            print("   âŒ 465 ç«¯å£è®¤è¯å¤±è´¥ï¼ˆå¯èƒ½æ˜¯æˆæƒç é”™è¯¯ï¼‰")
        except Exception as e465:
            print(f"   âŒ 465 SSL è¿æ¥å¤±è´¥: {e465}")
    
    # 587 TLS æˆ–å…¶ä»–é‚®ç®±
    try:
        print(f"   å°è¯•ä½¿ç”¨ {smtp_port} ç«¯å£ TLS è¿æ¥...")
        server = smtplib.SMTP(smtp_server, smtp_port, timeout=15)
        server.ehlo()
        server.starttls()
        server.ehlo()
        server.login(sender_email, email_password)
        server.sendmail(sender_email, [recipient_email], msg.as_string())
        server.quit()
        print(f"   âœ… é‚®ä»¶å‘é€æˆåŠŸ ({smtp_port} TLS)")
        return True
    except smtplib.SMTPAuthenticationError:
        print(f"   âŒ {smtp_port} ç«¯å£è®¤è¯å¤±è´¥")
        print("   ğŸ’¡ è¯·æ£€æŸ¥:")
        print("      1. EMAIL_PASSWORD æ˜¯å¦ä¸ºæ­£ç¡®çš„æˆæƒç ï¼ˆéQQå¯†ç ï¼‰")
        print("      2. SMTPæœåŠ¡æ˜¯å¦å·²åœ¨é‚®ç®±è®¾ç½®ä¸­å¼€å¯")
        return False
    except Exception as e:
        print(f"   âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")
        print("   ğŸ’¡ è¯·æ£€æŸ¥:")
        print("      1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("      2. SMTP_SERVER å’Œ SMTP_PORT æ˜¯å¦æ­£ç¡®")
        print("      3. é‚®ç®±SMTPæœåŠ¡æ˜¯å¦å·²å¼€å¯")
        return False

def main():
    print("=" * 60)
    print("ğŸš€ å¼€å§‹è´¢ç»æ–°é—»æŠ“å–å’Œæ¨é€")
    print(f"   æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # æŠ“å–æ–°é—»
    all_news = []
    
    print("\n[1/5] ä»RSSæºæŠ“å–æ–°é—»...")
    print("-" * 60)
    rss_news = fetch_news_from_rss()
    all_news.extend(rss_news)
    print(f"âœ“ RSSæºè·å– {len(rss_news)} æ¡æ–°é—»")
    print(f"  å…¶ä¸­é‡è¦æ–°é—» {sum(1 for n in rss_news if n.get('highlight'))} æ¡")
    
    print("\n[2/5] ä»NewsAPIæŠ“å–æ–°é—»...")
    print("-" * 60)
    api_news = fetch_news_from_api()
    all_news.extend(api_news)
    print(f"âœ“ APIè·å– {len(api_news)} æ¡æ–°é—»")
    print(f"  å…¶ä¸­é‡è¦æ–°é—» {sum(1 for n in api_news if n.get('highlight'))} æ¡")
    
    # å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœRSSå’ŒAPIéƒ½æ²¡æœ‰è·å–åˆ°è¶³å¤Ÿçš„æ–°é—»ï¼Œå°è¯•webçˆ¬è™«
    if len(all_news) < 5:
        print("\n[3/5] å¤‡ç”¨æ–¹æ¡ˆï¼šç½‘é¡µçˆ¬è™«...")
        print("-" * 60)
        web_news = fetch_news_from_web_scraping()
        all_news.extend(web_news)
        print(f"âœ“ ç½‘é¡µçˆ¬è™«è·å– {len(web_news)} æ¡æ–°é—»")
    else:
        print("\n[3/5] å¤‡ç”¨æ–¹æ¡ˆï¼šç½‘é¡µçˆ¬è™« (è·³è¿‡ï¼Œå·²æœ‰è¶³å¤Ÿæ–°é—»)")
    
    print("\n[4/5] æ ¼å¼åŒ–æ–°é—»å†…å®¹...")
    print("-" * 60)
    print(f"âœ“ æ€»å…±è·å– {len(all_news)} æ¡æ–°é—»")
    highlight_sum = sum(1 for n in all_news if n.get('highlight'))
    print(f"  å…¶ä¸­é‡è¦æ–°é—» {highlight_sum} æ¡")
    
    # æ ¼å¼åŒ–å†…å®¹
    content = format_news_content(all_news)
    
    # æ¨é€
    print("\n[5/5] æ‰§è¡Œæ¨é€...")
    print("-" * 60)
    
    print("\nğŸ“± å¾®ä¿¡æ¨é€:")
    send_via_serverchan(content)
    
    print("\nğŸ“§ é‚®ä»¶æ¨é€:")
    recipient_email = os.getenv('EMAIL_RECIPIENT', '')
    send_email(content, recipient_email)
    
    print("\n" + "=" * 60)
    print("âœ… ä»»åŠ¡å®Œæˆï¼")
    print("=" * 60)

if __name__ == '__main__':
    main()

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è´¢ç»æ–°é—»æŠ“å–å’Œæ¨é€è„šæœ¬
æŠ“å–è¿‡å»24å°æ—¶çš„è´¢ç»æ–°é—»ï¼Œé€šè¿‡Serveré…±æ¨é€åˆ°å¾®ä¿¡ï¼Œå¹¶å‘é€é‚®ä»¶
"""

import requests
import json
from datetime import datetime, timedelta
import os
import smtplib
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.header import Header
import feedparser
import re
import time
from urllib.parse import quote

# è¯·æ±‚å¤´ï¼Œé¿å…è¢«éƒ¨åˆ†ç«™ç‚¹æ‹’ç»
REQUEST_HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'application/rss+xml, application/xml, text/xml, */*',
}

def translate_to_chinese(text):
    """ä½¿ç”¨ç™¾åº¦ç¿»è¯‘APIå°†è‹±æ–‡ç¿»è¯‘æˆä¸­æ–‡ï¼ˆç®€æ˜“ç‰ˆæœ¬ï¼‰"""
    if not text or len(text.strip()) == 0:
        return text
    
    # å¦‚æœå·²ç»æœ‰ä¸­æ–‡å­—ç¬¦ï¼Œç›´æ¥è¿”å›
    if any('\u4e00' <= char <= '\u9fff' for char in text):
        return text
    
    try:
        # ä½¿ç”¨Googleç¿»è¯‘çš„å…è´¹æ¥å£ï¼ˆéå®˜æ–¹ï¼Œä½œä¸ºå¤‡é€‰æ–¹æ¡ˆï¼‰
        url = 'https://translate.googleapis.com/translate_a/single'
        params = {
            'client': 'gtx',
            'sl': 'en',
            'tl': 'zh-CN',
            'dt': 't',
            'q': text[:500]  # é™åˆ¶é•¿åº¦
        }
        
        response = requests.get(url, params=params, timeout=5)
        if response.status_code == 200:
            result = response.json()
            if result and len(result) > 0 and len(result[0]) > 0:
                translated = ''.join([item[0] for item in result[0] if item[0]])
                return translated if translated else text
    except Exception as e:
        print(f"  ç¿»è¯‘å¤±è´¥: {str(e)[:30]}ï¼Œä¿ç•™åŸæ–‡")
    
    return text

def classify_news(title, description, source):
    """å°†æ–°é—»åˆ†ä¸ºä¸‰ç±»ï¼š1.è´¢ç»é‡è¦æ–°é—» 2.é‡å¤§å›½é™…æ–°é—» 3.ç‰¹æœ—æ™®ç›¸å…³"""
    content = (title + ' ' + description).lower()
    
    # ç¬¬ä¸‰ç±»ï¼šç‰¹æœ—æ™®ç›¸å…³ï¼ˆæœ€ä¼˜å…ˆï¼‰
    trump_keywords = [
        'trump', 'donald trump', 'president trump', 'trump administration',
        'ç‰¹æœ—æ™®', 'trump says', 'trump announces', 'trump policy'
    ]
    if any(kw in content for kw in trump_keywords):
        # è¿›ä¸€æ­¥åˆ¤æ–­æ˜¯å¦ä¸é‡‘èå¸‚åœºç›¸å…³
        market_keywords = [
            'stock', 'market', 'trade', 'tariff', 'currency', 'dollar', 'gold',
            'fed', 'interest rate', 'economy', 'economic', 'wall street',
            'treasury', 'tax', 'fiscal', 'policy', 'china', 'regulation'
        ]
        if any(kw in content for kw in market_keywords):
            return 'trump'  # ç‰¹æœ—æ™®+å¸‚åœºå½±å“
    
    # ç¬¬äºŒç±»ï¼šé‡å¤§å›½é™…æ–°é—»ï¼ˆåœ°ç¼˜æ”¿æ²»ã€èƒ½æºã€è´§å¸ï¼‰
    geopolitical_keywords = [
        # åœ°åŒº
        'middle east', 'iran', 'israel', 'saudi', 'opec', 'russia', 'ukraine',
        'china', 'taiwan', 'north korea', 'syria', 'iraq',
        'ä¸­ä¸œ', 'ä¼Šæœ—', 'ä»¥è‰²åˆ—', 'ä¿„ç½—æ–¯', 'ä¹Œå…‹å…°',
        
        # èƒ½æºå’Œå¤§å®—å•†å“
        'oil', 'crude', 'petroleum', 'energy', 'opec', 'brent',
        'natural gas', 'commodity', 'commodities',
        'åŸæ²¹', 'çŸ³æ²¹', 'èƒ½æº',
        
        # è´§å¸å’Œæ±‡ç‡
        'dollar', 'yuan', 'euro', 'currency', 'exchange rate', 'forex',
        'ç¾å…ƒ', 'äººæ°‘å¸', 'æ¬§å…ƒ', 'æ±‡ç‡',
        
        # åœ°ç¼˜æ”¿æ²»äº‹ä»¶
        'war', 'conflict', 'sanction', 'embargo', 'military', 'nuclear',
        'geopolitical', 'crisis', 'tension',
        'æˆ˜äº‰', 'å†²çª', 'åˆ¶è£', 'å†›äº‹', 'å±æœº'
    ]
    if any(kw in content for kw in geopolitical_keywords):
        return 'international'  # é‡å¤§å›½é™…æ–°é—»
    
    # ç¬¬ä¸€ç±»ï¼šè´¢ç»é‡è¦æ–°é—»ï¼ˆè‚¡å¸‚ã€å¤®è¡Œã€ç»æµæ•°æ®ï¼‰
    financial_keywords = [
        # è‚¡å¸‚
        'stock market', 'stock', 'dow jones', 'nasdaq', 's&p 500', 's&p',
        'shanghai', 'shenzhen', 'hang seng', 'nikkei', 'ftse',
        'bull market', 'bear market', 'rally', 'crash', 'volatility',
        'è‚¡å¸‚', 'è‚¡ç¥¨', 'ä¸Šè¯', 'æ·±è¯', 'æ’æŒ‡',
        
        # å¤®è¡Œå’Œè´§å¸æ”¿ç­–
        'federal reserve', 'fed', 'central bank', 'interest rate',
        'monetary policy', 'inflation', 'deflation', 'rate cut', 'rate hike',
        'quantitative easing', 'qe', 'tightening',
        'ç¾è”å‚¨', 'å¤®è¡Œ', 'åˆ©ç‡', 'é€šèƒ€', 'åŠ æ¯', 'é™æ¯',
        
        # ç»æµæŒ‡æ ‡
        'gdp', 'employment', 'unemployment', 'jobs report', 'cpi', 'ppi',
        'retail sales', 'consumer confidence', 'pmi', 'manufacturing',
        'recession', 'growth', 'economic data',
        'å°±ä¸š', 'å¤±ä¸š', 'GDP', 'ç»æµå¢é•¿',
        
        # ä¼ä¸šè´¢æŠ¥
        'earnings', 'revenue', 'profit', 'quarterly results', 'eps',
        'guidance', 'forecast', 'outlook',
        'è´¢æŠ¥', 'ç›ˆåˆ©', 'è¥æ”¶'
    ]
    if any(kw in content for kw in financial_keywords):
        return 'financial'  # è´¢ç»é‡è¦æ–°é—»
    
    # é»˜è®¤å½’ä¸ºè´¢ç»æ–°é—»
    return 'financial'

def fetch_news_from_rss():
    """ä»RSSæºæŠ“å–è´¢ç»æ–°é—»ï¼ˆè´¢ç»æºç›´æ¥å–æœ€æ–°æ¡ç›®ï¼Œä¸ä¾èµ–å…³é”®è¯è¿‡æ»¤ï¼‰"""
    news_list = []
    
    # RSSæºåˆ—è¡¨ - ä½¿ç”¨å…¨çƒå¯è®¿é—®çš„ã€ç¨³å®šçš„æº
    rss_sources = [
        # CNBC (ç¾å›½è´¢ç»ç½‘ç»œç”µè§†)
        {'url': 'https://www.cnbc.com/id/100003114/device/rss/rss.html', 'source': 'CNBC', 'priority': 1},
        
        # Yahoo Finance
        {'url': 'https://feeds.finance.yahoo.com/rss/2.0/headline', 'source': 'Yahoo Finance', 'priority': 1},
        
        # Investor's Business Daily
        {'url': 'https://feeds.investors.com/feeds/ibd-top-10.xml', 'source': 'IBD', 'priority': 1},
        
        # Financial Express (å°åº¦è´¢ç»)
        {'url': 'https://www.financialexpress.com/feed/', 'source': 'Financial Express', 'priority': 2},
        
        # Business Insider
        {'url': 'https://feeds.businessinsider.com/markets/news', 'source': 'Business Insider', 'priority': 2},
        
        # The Motley Fool
        {'url': 'https://feeds.fool.com/foolscoop/index.xml', 'source': 'The Motley Fool', 'priority': 2},
    ]
    
    # å¼ºåŒ–å…³é”®è¯ï¼ˆä¸­è‹±æ–‡æ··åˆï¼Œé€‚åº”å›½é™…è´¢ç»æ–°é—»ï¼‰
    keywords = [
        # å¤®è¡Œç›¸å…³
        'ç¾è”å‚¨', 'Federal Reserve', 'Fed', 'interest rate', 'å¤®è¡Œ', 'central bank', 'monetary policy', 'é™æ¯', 'åŠ æ¯',
        
        # æ”¿æ²»ç»æµ
        'Trump', 'ç‰¹æœ—æ™®', 'President', 'æ€»ç»Ÿ', 'æ”¿åºœ', 'government', 'policy', 'æ”¿ç­–',
        
        # è‚¡å¸‚
        'stock', 'è‚¡ç¥¨', 'stock market', 'è‚¡å¸‚', 'index', 'æŒ‡æ•°', 'Nasdaq', 'NYSE', 'Dow Jones',
        'S&P 500', 'ä¸Šè¯', 'æ·±è¯', 'åˆ›ä¸šæ¿', 'Shanghai', 'Shenzhen',
        
        # ç»æµæŒ‡æ ‡
        'inflation', 'é€šèƒ€', 'GDP', 'unemployment', 'å°±ä¸š', 'CPI', 'PPI', 'ç»æµ',
        'earnings', 'ç›ˆåˆ©', 'revenue', 'æ”¶å…¥', 'profit', 'åˆ©æ¶¦',
        
        # è¡Œä¸šä¸å…¬å¸
        'Tesla', 'Apple', 'Microsoft', 'Amazon', 'Google',
        'Bitcoin', 'æ¯”ç‰¹å¸', 'crypto', 'åŠ å¯†', 'technology', 'ç§‘æŠ€',
        'energy', 'èƒ½æº', 'oil', 'æ²¹ä»·', 'banking', 'é“¶è¡Œ',
        
        # æ±‡ç‡å¤–æ±‡
        'exchange rate', 'æ±‡ç‡', 'yuan', 'äººæ°‘å¸', 'dollar', 'euro', 'æ¬§å…ƒ',
        
        # å€ºåˆ¸å¸‚åœº
        'bond', 'å€ºåˆ¸', 'Treasury', 'å›½å€º', 'yield', 'æ”¶ç›Šç‡',
        
        # è´¸æ˜“ä¸å…³ç¨
        'trade', 'è´¸æ˜“', 'tariff', 'å…³ç¨', 'commerce', 'å•†ä¸š',
        
        # æˆ¿åœ°äº§
        'real estate', 'æˆ¿åœ°äº§', 'property', 'æˆ¿äº§', 'housing', 'ä½æˆ¿',
        
        # å…¶ä»–å¸¸è§è¯
        'market', 'å¸‚åœº', 'price', 'ä»·æ ¼', 'rise', 'ä¸Šå‡', 'fall', 'ä¸‹é™',
        'gain', 'loss', 'æ¶¨', 'è·Œ', 'surge', 'é£™å‡', 'crash', 'å´©ç›˜'
    ]
    
    
    for rss_info in rss_sources:
        try:
            print(f"å°è¯•æŠ“å– {rss_info['source']} (ä¼˜å…ˆçº§{rss_info['priority']})...")
            # ç”¨ requests æ‹‰å–ï¼Œå¸¦ User-Agentï¼Œå†äº¤ç»™ feedparser è§£æ
            try:
                resp = requests.get(
                    rss_info['url'], 
                    headers=REQUEST_HEADERS, 
                    timeout=20,  # å¢åŠ è¶…æ—¶æ—¶é—´
                    verify=False  # å¿½ç•¥SSLéªŒè¯
                )
                resp.raise_for_status()
                resp.encoding = resp.apparent_encoding or 'utf-8'
                feed = feedparser.parse(resp.content)
            except requests.exceptions.Timeout:
                print(f"  {rss_info['source']} è¿æ¥è¶…æ—¶ï¼Œè·³è¿‡")
                continue
            except requests.exceptions.ConnectionError as e:
                print(f"  {rss_info['source']} ç½‘ç»œé”™è¯¯: {str(e)[:50]}...")
                continue
            except Exception as e:
                print(f"  {rss_info['source']} è·å–å¤±è´¥: {str(e)[:50]}...")
                continue
            
            if not feed.entries:
                print(f"  {rss_info['source']} æ— æ¡ç›®")
                continue
                
            current_time = datetime.now()
            taken = 0
            max_per_source = 15  # æ‰©å¤§å•æºè·å–æ•°é‡
            
            for entry in feed.entries:
                if taken >= max_per_source:
                    break
                title = entry.get('title', '').strip()
                if not title:
                    continue
                
                # è·å–å‘å¸ƒæ—¶é—´ï¼ˆå¯é€‰ï¼Œä½†ä¸å¼ºåˆ¶è¿‡æ»¤ï¼‰
                pub_time = None
                try:
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_time = datetime(*entry.published_parsed[:6])
                        # æ”¾å®½åˆ°72å°æ—¶ï¼Œä½†ä¼˜å…ˆæ˜¾ç¤º24å°æ—¶å†…çš„
                        if current_time - pub_time > timedelta(hours=72):
                            continue
                except Exception:
                    pass  # å¦‚æœæ— æ³•è§£ææ—¶é—´ï¼Œä»ç„¶ä¿ç•™
                
                summary = entry.get('summary', '') or entry.get('description', '')
                if summary and hasattr(summary, 'replace'):
                    summary = re.sub(r'<[^>]+>', '', summary)
                else:
                    summary = ''
                
                content = (title + ' ' + summary)
                # æ”¹è¿›å…³é”®è¯æ£€æµ‹ï¼šä»»æ„åŒ¹é…éƒ½ç®—"é‡è¦"
                is_highlight = any(kw in content for kw in keywords)
                # å¦‚æœæ²¡æœ‰å…³é”®è¯ä½†æœ‰å†…å®¹ï¼Œä»ç„¶ä¿ç•™ï¼ˆé‡è¦æ€§ç¨ä½ï¼‰
                
                # ç¿»è¯‘æ ‡é¢˜å’Œæè¿°
                title_cn = translate_to_chinese(title)
                description_cn = translate_to_chinese(summary[:200]) if summary else ''
                
                # åˆ†ç±»æ–°é—»
                category = classify_news(title, summary, rss_info['source'])
                
                news_item = {
                    'title': title_cn,
                    'title_en': title,
                    'description': description_cn,
                    'description_en': summary[:200] if summary else '',
                    'url': entry.get('link', ''),
                    'source': rss_info['source'],
                    'publishedAt': entry.get('published', ''),
                    'highlight': is_highlight,
                    'time_diff': (current_time - pub_time).total_seconds() / 3600 if pub_time else None,
                    'category': category,
                }
                news_list.append(news_item)
                taken += 1
            
            print(f"  è·å– {taken} æ¡æ–°é—»")
        except Exception as e:
            print(f"  {rss_info['source']} å¤„ç†å¼‚å¸¸: {str(e)[:50]}...")
            continue
    
    # è‹¥ä¸€æ¡éƒ½æ²¡æœ‰ï¼Œåšå…œåº•ï¼šä¸é™åˆ¶æ—¶é—´ï¼Œä»èƒ½ç”¨çš„æºå–æœ€æ–°å‡ æ¡
    if not news_list:
        print("\nâš ï¸ ä¸»åˆ—è¡¨ä¸ºç©ºï¼Œæ‰§è¡Œå…œåº•ç­–ç•¥...")
        for rss_info in rss_sources:
            try:
                print(f"  å…œåº•å°è¯•: {rss_info['source']}")
                resp = requests.get(
                    rss_info['url'], 
                    headers=REQUEST_HEADERS, 
                    timeout=20,
                    verify=False
                )
                resp.raise_for_status()
                resp.encoding = resp.apparent_encoding or 'utf-8'
                feed = feedparser.parse(resp.content)
                
                for entry in (feed.entries or [])[:20]:  # å…œåº•å¢åŠ åˆ°20æ¡
                    title = entry.get('title', '').strip()
                    if not title:
                        continue
                    summary = entry.get('summary', '') or entry.get('description', '')
                    if summary and hasattr(summary, 'replace'):
                        summary = re.sub(r'<[^>]+>', '', summary)
                    else:
                        summary = ''
                    title_cn = translate_to_chinese(title)
                    description_cn = translate_to_chinese(summary[:200]) if summary else ''
                    category = classify_news(title, summary, rss_info['source'])
                    
                    news_list.append({
                        'title': title_cn,
                        'title_en': title,
                        'description': description_cn,
                        'description_en': summary[:200] if summary else '',
                        'url': entry.get('link', ''),
                        'source': rss_info['source'],
                        'publishedAt': entry.get('published', ''),
                        'highlight': False,
                        'time_diff': None,
                        'category': category,
                    })
                
                if news_list:
                    print(f"  âœ“ å…œåº•è·å– {len(news_list)} æ¡æ–°é—»")
                    break
            except Exception as e:
                print(f"  âœ— å…œåº•å¤±è´¥ {rss_info['source']}: {str(e)[:30]}...")
                continue
    
    # ä¼˜å…ˆå±•ç¤ºå«å…³é”®è¯çš„ï¼Œå†æŒ‰æ—¶é—´æ’åº
    news_list.sort(key=lambda x: (
        not x.get('highlight', False),  # ä¼˜å…ˆæ˜¾ç¤ºæœ‰å…³é”®è¯çš„
        -(x.get('time_diff', 999) or 999)  # å†æŒ‰æ—¶é—´æ–°æ—§æ’åº
    ))
    
    return news_list

def fetch_news_from_web_scraping():
    """å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥ä»ç½‘é¡µçˆ¬å–è´¢ç»æ–°é—»"""
    news_list = []
    
    # çˆ¬å–çš„ç½‘ç«™åˆ—è¡¨ - å…¨çƒå¯è®¿é—®çš„è´¢ç»ç½‘ç«™
    scrape_sources = [
        {
            'url': 'https://news.google.com/topics/CAAqKggKEhAP-nZ_GqJEFQryfS9NqMEsqAEwkqKBBigBKkAP-nZ_GqJEFQryfS9NqMEsqAEwkqKBBg',
            'name': 'Google News - Business',
            'parser': 'simple'
        },
        {
            'url': 'https://www.cnbc.com/markets/',
            'name': 'CNBC Markets',
            'parser': 'simple'
        },
        {
            'url': 'https://www.bloomberg.com/markets',
            'name': 'Bloomberg Markets',
            'parser': 'simple'
        }
    ]
    
    from bs4 import BeautifulSoup
    
    for source in scrape_sources:
        try:
            print(f"  å°è¯•çˆ¬å– {source['name']}...")
            response = requests.get(
                source['url'],
                headers=REQUEST_HEADERS,
                timeout=15,
                verify=False
            )
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # é€šç”¨æ–°é—»é¡¹æå–
            articles = soup.find_all('article')[:10] or soup.find_all('a', {'data-article-id': True})[:10]
            
            for article in articles[:10]:
                try:
                    title = None
                    link = None
                    
                    # å°è¯•å¤šç§é€‰æ‹©å™¨
                    title_elem = article.find('h2') or article.find('h3') or article.find('a')
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                    
                    if not title:
                        continue
                    
                    link_elem = article.find('a', href=True)
                    if link_elem:
                        link = link_elem['href']
                        if not link.startswith('http'):
                            link = 'https://' + source['url'].split('/')[2] + link
                    
                    if title and len(title) > 5:
                        title_cn = translate_to_chinese(title)
                        category = classify_news(title, '', source['name'])
                        
                        news_list.append({
                            'title': title_cn,
                            'title_en': title[:200],
                            'description': '',
                            'description_en': '',
                            'url': link or '',
                            'source': source['name'],
                            'publishedAt': '',
                            'highlight': True,
                            'time_diff': 0,
                            'category': category,
                        })
                except Exception:
                    continue
            
            if len(news_list) >= 5:
                print(f"  âœ“ çˆ¬å–åˆ° {len(news_list)} æ¡æ–°é—»")
                break
        except Exception as e:
            print(f"  âœ— çˆ¬å–å¤±è´¥: {str(e)[:30]}...")
            continue
    
    return news_list

def fetch_news_from_api():
    """ä»NewsAPIæŠ“å–æ–°é—»ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰- å‚è€ƒ https://newsapi.org/ å®˜æ–¹æ–‡æ¡£"""
    news_list = []
    api_key = os.getenv('NEWS_API_KEY', '')
    
    if not api_key:
        print("  âš ï¸ æœªé…ç½®NEWS_API_KEYï¼Œè·³è¿‡APIæ–°é—»æº")
        print("  è·å–APIå¯†é’¥ï¼šhttps://newsapi.org/register")
        return news_list
    
    try:
        print("  æ­£åœ¨ä»NewsAPIè·å–è´¢ç»æ–°é—»...")
        
        # ä½¿ç”¨ top-headlines ç«¯ç‚¹è·å–å¤´æ¡æ–°é—»ï¼ˆæ›´é€‚åˆè´¢ç»æ—©æŠ¥ï¼‰
        url = "https://newsapi.org/v2/top-headlines"
        params = {
            'apiKey': api_key,
            'category': 'business',  # å•†ä¸š/è´¢ç»ç±»åˆ«
            'language': 'en',        # è‹±æ–‡æ–°é—»
            'pageSize': 20           # è·å–20æ¡
        }
        
        response = requests.get(url, params=params, timeout=15)
        
        if response.status_code == 200:
            data = response.json()
            if data.get('status') == 'ok':
                articles = data.get('articles', [])
                print(f"  âœ“ NewsAPIè¿”å› {len(articles)} æ¡æ–°é—»")
                
                for article in articles:
                    title = article.get('title', '')
                    if not title or title == '[Removed]':
                        continue
                    
                    # è®¡ç®—å‘å¸ƒæ—¶é—´å·®
                    pub_time = None
                    time_diff = None
                    try:
                        pub_str = article.get('publishedAt', '')
                        if pub_str:
                            pub_time = datetime.strptime(pub_str, '%Y-%m-%dT%H:%M:%SZ')
                            time_diff = (datetime.now() - pub_time).total_seconds() / 3600
                    except Exception:
                        pass
                    
                    title_cn = translate_to_chinese(title)
                    desc = article.get('description', '')[:200] if article.get('description') else ''
                    description_cn = translate_to_chinese(desc)
                    category = classify_news(title, desc, 'NewsAPI')
                    
                    news_list.append({
                        'title': title_cn,
                        'title_en': title,
                        'description': description_cn,
                        'description_en': desc,
                        'url': article.get('url', ''),
                        'source': article.get('source', {}).get('name', 'NewsAPI'),
                        'publishedAt': article.get('publishedAt', ''),
                        'highlight': True,
                        'time_diff': time_diff,
                        'category': category,
                    })
            else:
                error_msg = data.get('message', 'æœªçŸ¥é”™è¯¯')
                print(f"  âœ— NewsAPIé”™è¯¯: {error_msg}")
        elif response.status_code == 401:
            print(f"  âœ— NewsAPIè®¤è¯å¤±è´¥ - APIå¯†é’¥æ— æ•ˆ")
            print(f"  è¯·æ£€æŸ¥NEWS_API_KEYæ˜¯å¦æ­£ç¡®")
        elif response.status_code == 429:
            print(f"  âœ— NewsAPIè¯·æ±‚é™åˆ¶ - å·²è¶…è¿‡å…è´¹é…é¢")
        else:
            print(f"  âœ— NewsAPIè¿”å›é”™è¯¯: {response.status_code}")
            
    except Exception as e:
        print(f"  âœ— NewsAPIæŠ“å–å¼‚å¸¸: {str(e)[:50]}...")
    
    return news_list

def format_news_content(news_list):
    """æ ¼å¼åŒ–æ–°é—»å†…å®¹ä¸ºæ¨é€æ ¼å¼ - æŒ‰ä¸‰ç±»åˆ†ç±»æ˜¾ç¤º"""
    if not news_list:
        return "ğŸ“° è¿‡å»24å°æ—¶æœªå‘ç°é‡è¦è´¢ç»æ–°é—»ï¼Œä½†ç³»ç»Ÿæ­£å¸¸è¿è¡Œã€‚å¦‚æœæŒç»­æ— æ–°é—»ï¼Œè¯·æ£€æŸ¥RSSæºæ˜¯å¦å¯è®¿é—®ã€‚"
    
    # å»é‡ï¼ˆåŸºäºè‹±æ–‡æ ‡é¢˜ï¼Œé¿å…ç¿»è¯‘å·®å¼‚å¯¼è‡´çš„é‡å¤ï¼‰
    seen_titles = set()
    unique_news = []
    for news in news_list:
        title_en = news.get('title_en', news.get('title', ''))
        if title_en and title_en not in seen_titles:
            seen_titles.add(title_en)
            unique_news.append(news)
    
    # æŒ‰ç±»åˆ«åˆ†ç»„
    trump_news = [n for n in unique_news if n.get('category') == 'trump']
    international_news = [n for n in unique_news if n.get('category') == 'international']
    financial_news = [n for n in unique_news if n.get('category') == 'financial']
    
    # ç»Ÿè®¡ä¿¡æ¯
    content = f"ğŸ“Š è´¢ç»æ—©æŠ¥ - {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}\n"
    content += "=" * 60 + "\n\n"
    content += f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯\n"
    content += f"  â€¢ æ€»æ–°é—»æ•°ï¼š{len(unique_news)} æ¡\n"
    content += f"  â€¢ ç‰¹æœ—æ™®ç›¸å…³ï¼š{len(trump_news)} æ¡\n"
    content += f"  â€¢ é‡å¤§å›½é™…æ–°é—»ï¼š{len(international_news)} æ¡\n"
    content += f"  â€¢ è´¢ç»é‡è¦æ–°é—»ï¼š{len(financial_news)} æ¡\n"
    content += f"  â€¢ æ›´æ–°æ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    
    if len(unique_news) == 0:
        content += "âš ï¸ æš‚æ— æ–°é—»æ•°æ®\n"
        return content
    
    # ç¬¬ä¸€ç±»ï¼šç‰¹æœ—æ™®ç›¸å…³ï¼ˆé‡ç‚¹å…³æ³¨ï¼‰
    if trump_news:
        content += "ğŸ”´ ã€ç‰¹æœ—æ™®æœ€æ–°æ¶ˆæ¯ - å¸‚åœºå½±å“åˆ†æã€‘\n"
        content += "=" * 60 + "\n"
        content += "ï¼ˆå¯¹è‚¡å¸‚ã€å¤–æ±‡ã€é»„é‡‘å…·æœ‰é‡è¦å½±å“çš„æ¶ˆæ¯ï¼‰\n\n"
        
        for idx, news in enumerate(trump_news[:8], 1):  # æ˜¾ç¤ºå‰8æ¡
            title = news.get('title', 'æ— æ ‡é¢˜')
            description = news.get('description', '')
            url = news.get('url', '')
            source = news.get('source', 'æœªçŸ¥æ¥æº')
            time_info = _format_time_info(news.get('time_diff'))
            
            content += f"{idx}. ã€{source}ã€‘{title}{time_info}\n"
            if description:
                content += f"   ğŸ’¬ {description}\n"
            if url:
                content += f"   ğŸ”— {url}\n"
            content += "\n"
    
    # ç¬¬äºŒç±»ï¼šé‡å¤§å›½é™…æ–°é—»ï¼ˆåœ°ç¼˜æ”¿æ²»ã€èƒ½æºã€è´§å¸ï¼‰
    if international_news:
        content += "ğŸŒ ã€é‡å¤§å›½é™…æ–°é—»ã€‘\n"
        content += "=" * 60 + "\n"
        content += "ï¼ˆä¸­ä¸œå±€åŠ¿ã€èƒ½æºä»·æ ¼ã€è´§å¸æ±‡ç‡ã€åœ°ç¼˜æ”¿æ²»ï¼‰\n\n"
        
        for idx, news in enumerate(international_news[:8], 1):  # æ˜¾ç¤ºå‰8æ¡
            title = news.get('title', 'æ— æ ‡é¢˜')
            description = news.get('description', '')
            url = news.get('url', '')
            source = news.get('source', 'æœªçŸ¥æ¥æº')
            time_info = _format_time_info(news.get('time_diff'))
            
            content += f"{idx}. ã€{source}ã€‘{title}{time_info}\n"
            if description:
                content += f"   ğŸ’¬ {description}\n"
            if url:
                content += f"   ğŸ”— {url}\n"
            content += "\n"
    
    # ç¬¬ä¸‰ç±»ï¼šè´¢ç»é‡è¦æ–°é—»ï¼ˆç¾è‚¡ã€ä¸­å›½è‚¡å¸‚ã€å¤®è¡Œæ”¿ç­–ï¼‰
    if financial_news:
        content += "ğŸ’° ã€è´¢ç»é‡è¦æ–°é—»ã€‘\n"
        content += "=" * 60 + "\n"
        content += "ï¼ˆç¾å›½è‚¡å¸‚ã€ä¸­å›½è‚¡å¸‚ã€å¤®è¡Œæ”¿ç­–ã€ç»æµæ•°æ®ï¼‰\n\n"
        
        for idx, news in enumerate(financial_news[:10], 1):  # æ˜¾ç¤ºå‰10æ¡
            title = news.get('title', 'æ— æ ‡é¢˜')
            description = news.get('description', '')
            url = news.get('url', '')
            source = news.get('source', 'æœªçŸ¥æ¥æº')
            time_info = _format_time_info(news.get('time_diff'))
            
            content += f"{idx}. ã€{source}ã€‘{title}{time_info}\n"
            if description and len(description) > 10:
                content += f"   ğŸ’¬ {description}\n"
            if url:
                content += f"   ğŸ”— {url}\n"
            content += "\n"
    
    content += "=" * 60 + "\n"
    content += "âœ¨ æ•°æ®æ¥æºï¼šCNBCã€Yahoo Financeã€NewsAPIç­‰å›½é™…è´¢ç»åª’ä½“\n"
    content += "ğŸ¤– ç”±AIè‡ªåŠ¨æŠ“å–ã€ç¿»è¯‘ã€åˆ†ç±»æ•´ç†\n"
    
    return content

def _format_time_info(time_diff):
    """æ ¼å¼åŒ–æ—¶é—´ä¿¡æ¯"""
    if time_diff is None:
        return ""
    
    hours = int(time_diff)
    if hours < 1:
        return " (åˆšåˆš)"
    elif hours < 24:
        return f" ({hours}å°æ—¶å‰)"
    else:
        days = hours // 24
        return f" ({days}å¤©å‰)"

def send_via_serverchan(content):
    """é€šè¿‡Serveré…±APIæ¨é€åˆ°å¾®ä¿¡"""
    send_key = os.getenv('SERVERCHAN_SEND_KEY', '')
    
    if not send_key:
        print("âš ï¸ æœªé…ç½®SERVERCHAN_SEND_KEYï¼Œè·³è¿‡å¾®ä¿¡æ¨é€")
        print("   è¯·åœ¨GitHub Secretsä¸­æ·»åŠ  SERVERCHAN_SEND_KEY")
        print("   è·å–æ–¹å¼ï¼šè®¿é—® https://sctapi.ftqq.com/ å¾®ä¿¡æ‰«ç ç™»å½•è·å–")
        return False
    
    # Serveré…±æœ€æ–°APIåœ°å€
    url = f"https://sctapi.ftqq.com/{send_key}.send"
    
    title = f"ğŸ“Š è´¢ç»æ—©æŠ¥ - {datetime.now().strftime('%mæœˆ%dæ—¥')}"
    
    data = {
        'title': title,
        'desp': content
    }
    
    try:
        print(f"æ­£åœ¨æ¨é€åˆ°Serveré…±: {url[:50]}...")
        response = requests.post(url, data=data, timeout=10)
        result = response.json()
        
        if response.status_code == 200 and result.get('code') == 0:
            print("âœ… å¾®ä¿¡æ¨é€æˆåŠŸ - å·²å‘é€åˆ°å¾®ä¿¡")
            return True
        else:
            error_msg = result.get('message', 'æœªçŸ¥é”™è¯¯')
            error_code = result.get('code', 'N/A')
            print(f"âŒ å¾®ä¿¡æ¨é€å¤±è´¥")
            print(f"   é”™è¯¯ä»£ç : {error_code}")
            print(f"   é”™è¯¯ä¿¡æ¯: {error_msg}")
            
            # å¸¸è§é”™è¯¯è¯Šæ–­
            if error_code == 40001:
                print("   ğŸ’¡ æç¤ºï¼šSendKeyå¯èƒ½å·²è¿‡æœŸï¼Œè¯·é‡æ–°è·å–")
            elif error_code == 40002:
                print("   ğŸ’¡ æç¤ºï¼šSendKeyæ ¼å¼é”™è¯¯")
            
            return False
    except Exception as e:
        print(f"âŒ å¾®ä¿¡æ¨é€å¼‚å¸¸: {e}")
        print(f"   è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–Serveré…±æœåŠ¡æ˜¯å¦å¯ç”¨")
        return False

def send_email(content, recipient_email):
    """å‘é€é‚®ä»¶ï¼ˆQQé‚®ç®±ä¼˜å…ˆä½¿ç”¨465ç«¯å£SSLï¼Œå¤±è´¥åˆ™å°è¯•587 TLSï¼‰"""
    sender_email = os.getenv('EMAIL_SENDER', '')
    email_password = os.getenv('EMAIL_PASSWORD', '')
    smtp_server = os.getenv('SMTP_SERVER', 'smtp.qq.com')
    smtp_port = int(os.getenv('SMTP_PORT', '587'))
    
    if not sender_email or not email_password:
        print("âš ï¸ æœªé…ç½®é‚®ç®±ä¿¡æ¯")
        print("   éœ€è¦é…ç½®çš„ç¯å¢ƒå˜é‡:")
        print("   â€¢ EMAIL_SENDER: å‘é€è€…é‚®ç®±åœ°å€")
        print("   â€¢ EMAIL_PASSWORD: é‚®ç®±å¯†ç æˆ–æˆæƒç ")
        print("   â€¢ EMAIL_RECIPIENT: æ¥æ”¶è€…é‚®ç®±åœ°å€")
        print("   â€¢ SMTP_SERVER: SMTPæœåŠ¡å™¨åœ°å€ (é»˜è®¤: smtp.qq.com)")
        print("   â€¢ SMTP_PORT: SMTPæœåŠ¡å™¨ç«¯å£ (é»˜è®¤: 587)")
        return False
    
    if not recipient_email:
        recipient_email = sender_email  # é»˜è®¤å‘é€ç»™è‡ªå·±
        print(f"ğŸ“§ å‘é€ç»™: {recipient_email} (é»˜è®¤å‘é€ç»™è‡ªå·±)")
    else:
        print(f"ğŸ“§ å‘é€ç»™: {recipient_email}")
    
    subject = f"è´¢ç»æ—©æŠ¥ - {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}"
    html_content = content.replace('\n', '<br>')
    msg = MIMEMultipart()
    msg['From'] = sender_email
    msg['To'] = recipient_email
    msg['Subject'] = Header(subject, 'utf-8')
    msg.attach(MIMEText(html_content, 'html', 'utf-8'))
    
    print(f"   SMTPæœåŠ¡å™¨: {smtp_server}:{smtp_port}")
    
    # å…ˆå°è¯• 465 ç«¯å£ SSLï¼ˆQQé‚®ç®±æ›´ç¨³å®šï¼‰
    if smtp_server == 'smtp.qq.com':
        try:
            print("   å°è¯•ä½¿ç”¨ 465 ç«¯å£ SSL è¿æ¥...")
            server = smtplib.SMTP_SSL(smtp_server, 465, timeout=15)
            server.login(sender_email, email_password)
            server.sendmail(sender_email, [recipient_email], msg.as_string())
            server.quit()
            print("   âœ… é‚®ä»¶å‘é€æˆåŠŸ (465 SSL)")
            return True
        except smtplib.SMTPAuthenticationError:
            print("   âŒ 465 ç«¯å£è®¤è¯å¤±è´¥ï¼ˆå¯èƒ½æ˜¯æˆæƒç é”™è¯¯ï¼‰")
        except Exception as e465:
            print(f"   âŒ 465 SSL è¿æ¥å¤±è´¥: {e465}")
    
    # 587 TLS æˆ–å…¶ä»–é‚®ç®±
    try:
        print(f"   å°è¯•ä½¿ç”¨ {smtp_port} ç«¯å£ TLS è¿æ¥...")
        server = smtplib.SMTP(smtp_server, smtp_port, timeout=15)
        server.ehlo()
        server.starttls()
        server.ehlo()
        server.login(sender_email, email_password)
        server.sendmail(sender_email, [recipient_email], msg.as_string())
        server.quit()
        print(f"   âœ… é‚®ä»¶å‘é€æˆåŠŸ ({smtp_port} TLS)")
        return True
    except smtplib.SMTPAuthenticationError:
        print(f"   âŒ {smtp_port} ç«¯å£è®¤è¯å¤±è´¥")
        print("   ğŸ’¡ è¯·æ£€æŸ¥:")
        print("      1. EMAIL_PASSWORD æ˜¯å¦ä¸ºæ­£ç¡®çš„æˆæƒç ï¼ˆéQQå¯†ç ï¼‰")
        print("      2. SMTPæœåŠ¡æ˜¯å¦å·²åœ¨é‚®ç®±è®¾ç½®ä¸­å¼€å¯")
        return False
    except Exception as e:
        print(f"   âŒ é‚®ä»¶å‘é€å¤±è´¥: {e}")
        print("   ğŸ’¡ è¯·æ£€æŸ¥:")
        print("      1. ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸")
        print("      2. SMTP_SERVER å’Œ SMTP_PORT æ˜¯å¦æ­£ç¡®")
        print("      3. é‚®ç®±SMTPæœåŠ¡æ˜¯å¦å·²å¼€å¯")
        return False

def main():
    print("=" * 60)
    print("ğŸš€ å¼€å§‹è´¢ç»æ–°é—»æŠ“å–å’Œæ¨é€")
    print(f"   æ‰§è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)
    
    # æŠ“å–æ–°é—»
    all_news = []
    
    print("\n[1/5] ä»RSSæºæŠ“å–æ–°é—»...")
    print("-" * 60)
    rss_news = fetch_news_from_rss()
    all_news.extend(rss_news)
    print(f"âœ“ RSSæºè·å– {len(rss_news)} æ¡æ–°é—»")
    
    print("\n[2/5] ä»NewsAPIæŠ“å–æ–°é—»...")
    print("-" * 60)
    api_news = fetch_news_from_api()
    all_news.extend(api_news)
    print(f"âœ“ APIè·å– {len(api_news)} æ¡æ–°é—»")
    
    # å¤‡ç”¨æ–¹æ¡ˆï¼šå¦‚æœRSSå’ŒAPIéƒ½æ²¡æœ‰è·å–åˆ°è¶³å¤Ÿçš„æ–°é—»ï¼Œå°è¯•webçˆ¬è™«
    if len(all_news) < 5:
        print("\n[3/5] å¤‡ç”¨æ–¹æ¡ˆï¼šç½‘é¡µçˆ¬è™«...")
        print("-" * 60)
        web_news = fetch_news_from_web_scraping()
        all_news.extend(web_news)
        print(f"âœ“ ç½‘é¡µçˆ¬è™«è·å– {len(web_news)} æ¡æ–°é—»")
    else:
        print("\n[3/5] å¤‡ç”¨æ–¹æ¡ˆï¼šç½‘é¡µçˆ¬è™« (è·³è¿‡ï¼Œå·²æœ‰è¶³å¤Ÿæ–°é—»)")
    
    print("\n[4/5] æ ¼å¼åŒ–æ–°é—»å†…å®¹...")
    print("-" * 60)
    print(f"âœ“ æ€»å…±è·å– {len(all_news)} æ¡æ–°é—»")
    
    # æŒ‰ç±»åˆ«ç»Ÿè®¡
    trump_count = sum(1 for n in all_news if n.get('category') == 'trump')
    intl_count = sum(1 for n in all_news if n.get('category') == 'international')
    fin_count = sum(1 for n in all_news if n.get('category') == 'financial')
    
    print(f"  â€¢ ç‰¹æœ—æ™®ç›¸å…³ï¼š{trump_count} æ¡")
    print(f"  â€¢ é‡å¤§å›½é™…æ–°é—»ï¼š{intl_count} æ¡")
    print(f"  â€¢ è´¢ç»é‡è¦æ–°é—»ï¼š{fin_count} æ¡")
    print(f"  â„¹ï¸  æ‰€æœ‰æ–°é—»å·²è‡ªåŠ¨ç¿»è¯‘ä¸ºä¸­æ–‡")
    
    # æ ¼å¼åŒ–å†…å®¹
    content = format_news_content(all_news)
    
    # æ¨é€
    print("\n[5/5] æ‰§è¡Œæ¨é€...")
    print("-" * 60)
    
    print("\nğŸ“± å¾®ä¿¡æ¨é€:")
    send_via_serverchan(content)
    
    print("\nğŸ“§ é‚®ä»¶æ¨é€:")
    recipient_email = os.getenv('EMAIL_RECIPIENT', '')
    send_email(content, recipient_email)
    
    print("\n" + "=" * 60)
    print("âœ… ä»»åŠ¡å®Œæˆï¼")
    print("=" * 60)

if __name__ == '__main__':
    main()
